{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data to list\n",
    "def turnToList(string):  \n",
    "    f = open(string, 'r')\n",
    "    x = f.readlines()\n",
    "    #print(x)\n",
    "    articleVocab = x[0]\n",
    "    ls = articleVocab.split(\"', '\")\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = turnToList(\"Data/group1.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def makeBatches(data,batch_size):\n",
    "    cuts = len(data)/batch_size\n",
    "    cuts = math.ceil(cuts)\n",
    "    final_list = [data[i:i+cuts] for i in range(0,len(data),cuts)]\n",
    "    return final_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitches = makeBatches(training_data, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156473\n"
     ]
    }
   ],
   "source": [
    "print(len(bitches[28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(batch,n):\n",
    "    window = [batch[i:i+n] for i in range(0,len(batch))]\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = bitches[1]\n",
    "w1 = sliding_windows(batch1,5)\n",
    "batch2 = bitches[2]\n",
    "w2 = sliding_windows(batch2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rich', '.', '</s>', '<s>'], ['rich', '.', '</s>', '<s>', 'By'], ['.', '</s>', '<s>', 'By', '2018'], ['</s>', '<s>', 'By', '2018', ','], ['<s>', 'By', '2018', ',', 'the'], ['By', '2018', ',', 'the', 'richest'], ['2018', ',', 'the', 'richest', '400'], [',', 'the', 'richest', '400', 'Americans'], ['the', 'richest', '400', 'Americans', 'paid'], ['richest', '400', 'Americans', 'paid', 'lower'], ['400', 'Americans', 'paid', 'lower', 'taxes'], ['Americans', 'paid', 'lower', 'taxes', 'as'], ['paid', 'lower', 'taxes', 'as', 'a'], ['lower', 'taxes', 'as', 'a', 'share'], ['taxes', 'as', 'a', 'share', 'of'], ['as', 'a', 'share', 'of', 'income'], ['a', 'share', 'of', 'income', 'than'], ['share', 'of', 'income', 'than', 'any'], ['of', 'income', 'than', 'any', 'other'], ['income', 'than', 'any', 'other', 'income'], ['than', 'any', 'other', 'income', 'group'], ['any', 'other', 'income', 'group', 'including'], ['other', 'income', 'group', 'including', 'the'], ['income', 'group', 'including', 'the', 'poorest'], ['group', 'including', 'the', 'poorest', 'Americans'], ['including', 'the', 'poorest', 'Americans', ','], ['the', 'poorest', 'Americans', ',', 'according'], ['poorest', 'Americans', ',', 'according', 'to'], ['Americans', ',', 'according', 'to', 'Berkeley'], [',', 'according', 'to', 'Berkeley', 'economists'], ['according', 'to', 'Berkeley', 'economists', 'Emmanuel'], ['to', 'Berkeley', 'economists', 'Emmanuel', 'Saez'], ['Berkeley', 'economists', 'Emmanuel', 'Saez', 'and'], ['economists', 'Emmanuel', 'Saez', 'and', 'Gabriel'], ['Emmanuel', 'Saez', 'and', 'Gabriel', 'Zucman'], ['Saez', 'and', 'Gabriel', 'Zucman', '.'], ['and', 'Gabriel', 'Zucman', '.', '</s>'], ['Gabriel', 'Zucman', '.', '</s>', '<s>'], ['Zucman', '.', '</s>', '<s>', 'The'], ['.', '</s>', '<s>', 'The', 'environmental'], ['</s>', '<s>', 'The', 'environmental', 'movement'], ['<s>', 'The', 'environmental', 'movement', ','], ['The', 'environmental', 'movement', ',', 'bipartisan'], ['environmental', 'movement', ',', 'bipartisan', 'at'], ['movement', ',', 'bipartisan', 'at', 'its'], [',', 'bipartisan', 'at', 'its', 'start'], ['bipartisan', 'at', 'its', 'start', 'in'], ['at', 'its', 'start', 'in', '1969'], ['its', 'start', 'in', '1969', ','], ['start', 'in', '1969', ',', 'was']]\n"
     ]
    }
   ],
   "source": [
    "print(w1[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1 output\n",
    "training_sequences = {}\n",
    "training_sequences['Batch 1'] = [w1[i] for i in range(0,3)]\n",
    "training_sequences['Batch 2'] = [w2[i] for i in range(0,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Batch 1': [['the', 'rich', '.', '</s>', '<s>'], ['rich', '.', '</s>', '<s>', 'By'], ['.', '</s>', '<s>', 'By', '2018']], 'Batch 2': [['to', 'strike', 'Soleimani', ',', 'who'], ['strike', 'Soleimani', ',', 'who', 'was'], ['Soleimani', ',', 'who', 'was', 'visiting']]}\n"
     ]
    }
   ],
   "source": [
    "print(training_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Bengio Model Update Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IGNORE\n",
    "def load(words, window_size, vocab_size=None):\n",
    "\n",
    "    \n",
    "    window_size = p.window_size\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        x_train.append(words[i: i + window_size - 1])\n",
    "        y_train.append(words[i +  window_size - 1])\n",
    "    \n",
    "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
    "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
    "\n",
    "def load_zh(words, window_size, vocab_size=None):\n",
    "    window_size = p.window_size\n",
    "    \n",
    "    \n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        x_train.append(words[i: i + window_size - 1])\n",
    "        y_train.append(words[i +  window_size - 1])\n",
    "    \n",
    "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
    "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
    "            \n",
    "def convert_to_id(x_train, y_train, vocab):\n",
    "    \n",
    "    word_to_id = {}\n",
    "    for i, vocab in enumerate(vocab):\n",
    "        word_to_id[vocab] = i\n",
    "        \n",
    "    for i in range(len(x_train)):\n",
    "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
    "        y_train[i] = word_to_id[y_train[i][0]]\n",
    "        \n",
    "    return x_train.astype(int), y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-1f8d5284938a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_zh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab_size: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c4153e250d9f>\u001b[0m in \u001b[0;36mload_zh\u001b[0;34m(words, window_size, vocab_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mwindow_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "###IGNORE\n",
    "file = open('Data/bc_unbatched.pkl','rb')\n",
    "vocab_dict = pickle.load(file)\n",
    "window_size = p.window_size\n",
    "vocab_size = p.vocab_size\n",
    "x_raw, y_raw, vocab, word2id = load_zh(vocab_dict, window_size, vocab_size)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
    "print('Length: {}'.format(len(x_train)))\n",
    "print('Number of batch: {}'.format(len(x_train) / batch_size))\n",
    "###IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load Data from pickle file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1\n",
    "# #bring in pickle file for vocab dict\n",
    "# import pickle\n",
    "\n",
    "# file = open('Data/trainingDictPickle1.pkl','rb')\n",
    "# vocab_dict = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176510"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('Data/bc_unbatched.pkl','rb')\n",
    "vocab_dict = pickle.load(file)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    window_size = 5\n",
    "    num_batches = 30\n",
    "\n",
    "    vocab_size = len(vocab_dict)\n",
    "    batch_size = vocab_size/num_batches\n",
    "    hidden_units = 50\n",
    "    embeddings_dim = 60\n",
    "    num_epochs = 20\n",
    "\n",
    "    learning_rate = 0.5\n",
    "    context_window = 5\n",
    "    gpu_mem = 0.25\n",
    "\n",
    "    tf_precision = tf.float32\n",
    "    np_precision = np.float32\n",
    "\n",
    "    init_scale = 0.5\n",
    "    max_grad = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39217.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Parameters()\n",
    "p.batch_size\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initiliaze model with parameters to use in training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, params=Parameters()):\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #initialize input and output variables\n",
    "        self._y = tf.placeholder(dtype=tf.int32,shape=(params.num_batches, params.vocab_size),name=\"Y\")\n",
    "        self._x = tf.placeholder(tf.int32,shape=(params.num_batches, params.window_size),name=\"X\")\n",
    "        #word features/embeddings\n",
    "        self.C = tf.Variable(tf.truncated_normal(shape=(params.vocab_size, params.embeddings_dim),mean=-1,stddev=-1),dtype=params.tf_precision,name=\"C\")\n",
    "        #projection to output weight\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(params.vocab_size, (params.window_size) * params.embeddings_dim)),dtype=params.tf_precision,name=\"W\")\n",
    "        #hidden layer weight and bias\n",
    "        self.H = tf.Variable(tf.random_normal(shape=(params.hidden_units, (params.window_size) * params.embeddings_dim)),dtype=params.tf_precision,name=\"H\")\n",
    "        self.d = tf.Variable(tf.random_normal(shape=(params.hidden_units,)),dtype=params.tf_precision,name=\"d\")\n",
    "        #hidden layer to output weight and bias\n",
    "        self.U = tf.Variable(tf.random_normal((params.vocab_size, params.hidden_units)),name=\"U\",dtype=params.tf_precision)\n",
    "        self.b = tf.Variable(tf.random_normal(shape=(params.vocab_size, )),dtype=params.tf_precision,name=\"b\")\n",
    "        #do calculations\n",
    "        # y = b + Wx + Utanh(d + Hx)\n",
    "        # x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
    "        with tf.name_scope('Projection_Layer'):\n",
    "            x  = tf.nn.embedding_lookup(self.C, self._x) # (batch_size, window_size-1, emb_dim)\n",
    "            print(\"total x size\" + str(x.shape))\n",
    "            x  = tf.reshape(x, shape=(params.num_batches, (params.window_size ) * params.embeddings_dim))\n",
    "        with tf.name_scope('Hidden_Layer'):\n",
    "            Hx = tf.matmul(x, tf.transpose(self.H)) # (batch_size, hidden_size)\n",
    "            o  = tf.add(self.d, Hx) # (batch_size, hidden_size)\n",
    "            a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
    "        with tf.name_scope('Output_Layer'):\n",
    "            Ua = tf.matmul(a, tf.transpose(self.U)) # (batch_size, vocab_size)\n",
    "            Wx = tf.matmul(x, tf.transpose(self.W)) # (batch_size, vocab_size)\n",
    "            y_hat  = tf.nn.softmax(tf.clip_by_value(tf.add(self.b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
    "    #2 different ways to minimize loss/cost\n",
    "    #loss function defined from github repo\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self._y,logits=y_hat))\n",
    "            self.perplexity = tf.exp(self.cost)\n",
    "        print(\"cost \" + str(self.cost))\n",
    "        optimizer = tf.train.AdagradOptimizer(params.learning_rate).minimize(self.cost)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper function to train from batch to batch***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for training\n",
    "\n",
    "#get next batch\n",
    "def nxt(x_train, y_train, batch_size):\n",
    "    p = Parameters()\n",
    "    num_batch = p.num_batches\n",
    "    start = 0\n",
    "\n",
    "    for n in range(num_batch): \n",
    "        if n == (num_batch):\n",
    "            print(\"completed batching\")\n",
    "        else:\n",
    "            offset = int((n + 1) * ((len(x_train) + len(y_train))/num_batch))\n",
    "            \n",
    "            x_batch = x_train[start:offset]\n",
    "            y_batch = y_train[start:offset]\n",
    "            start += offset\n",
    "            yield x_batch, y_batch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "- x and y are set by splitting data before with split helper function\n",
    "- vocab is a unique vocab list of the corpus\n",
    "- model tunes for cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,vocab):\n",
    "    params = Parameters()\n",
    "    \n",
    "    epoch_size = params.num_epochs\n",
    "    batch_size = params.batch_size\n",
    "    x_train = x\n",
    "    y_train = y\n",
    "    modl = Model()\n",
    "    optimizer = modl.optimizer\n",
    "    cost = modl.cost\n",
    "    print('Length: {}'.format(len(x_train)))\n",
    "    print('Number of batch: {}'.format(len(x_train) / batch_size))\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        initializer.run()\n",
    "\n",
    "        step = 0\n",
    "        avg_loss = 0\n",
    "        for epoch in range(epoch_size):\n",
    "            for x_batch, y_batch in nxt(x_train, y_train, batch_size):\n",
    "                \n",
    "                if len(x_batch) != batch_size:\n",
    "                    print(len(x_batch))\n",
    "                   \n",
    "                    continue\n",
    "                #train by context windows\n",
    "                contextCheck = 0\n",
    "                xContext = []\n",
    "                yContext = []\n",
    "                \n",
    "                for x in range(len(x_batch)):\n",
    "                    if contextCheck == params.context_window:\n",
    "                        \n",
    "                        feed_dict = {modl._x: xContext, modl._y: yContext}\n",
    "\n",
    "                        fetches = [cost, optimizer]\n",
    "                        Cost, _ = sess.run(fetches, feed_dict)\n",
    "                        avg_cost += Cost\n",
    "\n",
    "                        print('Loss: {}'.format(avg_cost))\n",
    "                        contextCheck = 0\n",
    "                    else:\n",
    "                        xContext.append(x_batch[x])\n",
    "                        yContext.append(y_batch[x])\n",
    "                        contextCheck = contextCheck + 1\n",
    "                \n",
    "                if step % 100 == 0:\n",
    "                    print('Step {}, Loss: {}'.format(step, avg_cost / 100))\n",
    "                    avg_cost = 0\n",
    "\n",
    "                step += 1\n",
    "\n",
    "        print('Training Done.')\n",
    "        word_embedding = modl.C.eval()\n",
    "        print(\"---------- Word Embdeddings -------------\")\n",
    "        print(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Open unbated brown data and turn into array of integers like previous corpus***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn unbatched brown data into dictionary then into integer filled array\n",
    "diks_int = {}\n",
    "def dict_maker(lst):\n",
    "    a = {val : idx + 1 for idx, val in enumerate(lst)}\n",
    "    return a\n",
    "uniqDict = set([token for token in vocab_dict])\n",
    "unique_dictionary = dict_maker(uniqDict)\n",
    "def integer_representations(vocab_dictinary,vocab_dict):\n",
    "    integer_rep = []\n",
    "    for token in vocab_dict:\n",
    "        try: \n",
    "            vocab_dictinary[token] \n",
    "            integer_rep.append(vocab_dictinary[token])\n",
    "        except:\n",
    "            integer_rep.append(\"<unk>\")\n",
    "    return integer_rep\n",
    "\n",
    "uniqList = integer_representations(unique_dictionary,vocab_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split and Train the data using an 80,20 training split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total x size(30, 5, 60)\n",
      "cost Tensor(\"Cost_17/Mean:0\", shape=(), dtype=float32)\n",
      "Length: 941208\n",
      "Number of batch: 24.0\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (5,) for Tensor 'X_17:0', which has shape '(30, 5)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-76040ed352c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mxRaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myRaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxRaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myRaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-37b360942b5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, vocab)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                         \u001b[0mCost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                         \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mCost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (5,) for Tensor 'X_17:0', which has shape '(30, 5)'"
     ]
    }
   ],
   "source": [
    "#split and train the data\n",
    "def splitData(data):\n",
    "    st = len(data) * .8\n",
    "    x = data[:int(st)]\n",
    "    y = data[int(st):len(data)]\n",
    "    vocabDict = set([token for token in vocab_dict])\n",
    "    return x,y,vocabDict\n",
    "\n",
    "xRaw,yRaw,vDict = splitData(uniqList)\n",
    "train(xRaw,yRaw,vDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
